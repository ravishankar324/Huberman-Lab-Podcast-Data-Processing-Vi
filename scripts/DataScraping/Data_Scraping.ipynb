{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import io\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreiving the API keys and AWS Credentials from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyB8shHe_RNabXVIzPLpJw5m0cydR2NhGd8\n",
      "UC2D2CMWXMOVWx7giW1n3LIg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravis\\OneDrive\\Documents\\Youtube_Data_Scraping\\yt_env\\lib\\site-packages\\boto3\\compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIATCKAP54DTYM7Q743\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the API key and channel ID\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\").strip()\n",
    "channel_id = os.getenv(\"CHANNEL_ID\").strip()\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "#Initialize the S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region)\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "print(aws_access_key_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch Channel Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def channel_details(youtube, channel_id):\n",
    "    nextPageToken = None\n",
    "    while True: \n",
    "            # Fetch video ids from the specified channel using pagination\n",
    "        response_1 = youtube.channels().list(\n",
    "                id=channel_id,\n",
    "                part=\"snippet, contentDetails, statistics\",\n",
    "                pageToken=nextPageToken\n",
    "\n",
    "            ).execute()\n",
    "\n",
    "        nextPageToken = response_1.get('nextPageToken')\n",
    "\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "\n",
    "    return response_1\n",
    "channel_details = channel_details(youtube, channel_id)\n",
    "channel_details_df = pd.DataFrame(channel_details['items'])\n",
    "bucket_name = 'andrew-huberman-podcast-analytics'\n",
    "s3_key = 'channeldetails/channeldetails.csv'\n",
    "\n",
    "# Convert DataFrame to CSV in memory\n",
    "csv_buffer = io.StringIO()\n",
    "channel_details_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_key,\n",
    "    Body=csv_buffer.getvalue()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the video ids details from the channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'FHM9RBV0W8KQ7ATF',\n",
       "  'HostId': 'baq1F8yOMwD4XRtT6Edp1AxYt9PegCL8QcKPFcQX7VR3bC1b3OFyfT03ARu0nHFF8IsPZ2fmHVI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'baq1F8yOMwD4XRtT6Edp1AxYt9PegCL8QcKPFcQX7VR3bC1b3OFyfT03ARu0nHFF8IsPZ2fmHVI=',\n",
       "   'x-amz-request-id': 'FHM9RBV0W8KQ7ATF',\n",
       "   'date': 'Tue, 03 Dec 2024 23:05:42 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"92f800dad8d00fbd159791f336fe9201\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"92f800dad8d00fbd159791f336fe9201\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_videoids(youtube, channel_id):\n",
    "    nextPageToken = None\n",
    "    videoids = []\n",
    "    \n",
    "    while True: \n",
    "            # Fetch video ids from the specified channel using pagination\n",
    "        response_1 = youtube.search().list(\n",
    "                channelId=channel_id,\n",
    "                type = 'video',\n",
    "                part=\"id,snippet\",\n",
    "                maxResults = 50,\n",
    "                pageToken=nextPageToken\n",
    "\n",
    "            ).execute()\n",
    "\n",
    "        nextPageToken = response_1.get('nextPageToken')\n",
    "\n",
    "        for i in response_1['items']:\n",
    "            videoids.append(i['id']['videoId'])\n",
    "            \n",
    "\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "\n",
    "    return videoids\n",
    "\n",
    "# Get the total number of videos\n",
    "videoids = get_videoids(youtube, channel_id)\n",
    "# Create a DataFrame from the video IDs and save it to a CSV file with an index, overwriting the existing file\n",
    "videoids_df = pd.DataFrame(videoids, columns=['video_id'])\n",
    "bucket_name = 'andrew-huberman-podcast-analytics'\n",
    "s3_key = 'videoids/videoids.csv'\n",
    "\n",
    "# Convert DataFrame to CSV in memory\n",
    "csv_buffer_videoids = io.StringIO()\n",
    "videoids_df.to_csv(csv_buffer_videoids, index=True)\n",
    "\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_key,\n",
    "    Body=csv_buffer_videoids.getvalue()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch video details From using video ids of channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'GV9H2RHZTYEZ04TD',\n",
       "  'HostId': 'QNmQTULVv6nvlvgQtSB3JEsPPAetCxElmYpvsAR2HltCBzbV2yZjr8wwDbppV+TrvqGNi+GAPCHtAYPto9u9F4ftBOxdnwjYZilweZ8rSQA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'QNmQTULVv6nvlvgQtSB3JEsPPAetCxElmYpvsAR2HltCBzbV2yZjr8wwDbppV+TrvqGNi+GAPCHtAYPto9u9F4ftBOxdnwjYZilweZ8rSQA=',\n",
       "   'x-amz-request-id': 'GV9H2RHZTYEZ04TD',\n",
       "   'date': 'Tue, 03 Dec 2024 23:09:16 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"8bdfa4adb63e14f889ab88611128303c\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"8bdfa4adb63e14f889ab88611128303c\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_videos(youtube, video_ids):\n",
    "    all_videos = []\n",
    "    # Process in chunks of 50 IDs\n",
    "    for chunk in chunk_video_ids(video_ids, 50):\n",
    "        response = youtube.videos().list(\n",
    "            id=','.join(chunk),\n",
    "            part=\"snippet,contentDetails,statistics\"\n",
    "        ).execute()\n",
    "        all_videos.extend(response.get('items', []))\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "# Helper function to break video IDs into 50 \n",
    "def chunk_video_ids(video_ids, chunk_size=50):\n",
    "    for i in range(0, len(video_ids), chunk_size):\n",
    "        yield video_ids[i:i + chunk_size]\n",
    "\n",
    "  # Example IDs\n",
    "videos = get_videos(youtube, videoids)\n",
    "print(len(videos))\n",
    "videos_df = pd.DataFrame(videos)\n",
    "\n",
    "bucket_name = 'andrew-huberman-podcast-analytics'\n",
    "s3_key = 'videos/videos.csv'\n",
    "\n",
    "# Convert DataFrame to CSV in memory\n",
    "csv_buffer_videos = io.StringIO()\n",
    "videos_df.to_csv(csv_buffer_videos, index=False)\n",
    "\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_key,\n",
    "    Body=csv_buffer_videos.getvalue()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fetch comments for all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_comments(youtube, video_id):\n",
    "    # Fetch comments from the specified video\n",
    "    PageToken = None\n",
    "    comments_list = []\n",
    "    while True:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=PageToken\n",
    "        ).execute()\n",
    "\n",
    "        comments_list.extend(response['items'])\n",
    "\n",
    "        PageToken = response.get('nextPageToken')\n",
    "\n",
    "        if not PageToken:\n",
    "            break\n",
    "    return comments_list\n",
    "\n",
    "df = pd.read_csv('videoids/videoids.csv')\n",
    "\n",
    "bucket_name = 'andrew-huberman-podcast-analytics'\n",
    "for i in range(200, 280):\n",
    "    video_id = df.loc[i]['video_id']\n",
    "    comment_details = get_all_comments(youtube, video_id)\n",
    "    comments_df = pd.DataFrame(comment_details)\n",
    "\n",
    "    # Convert to parquet format in memory\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    comments_df.to_parquet(parquet_buffer)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_key = f'comments/{video_id}_{i}.parquet'\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=s3_key,\n",
    "        Body=parquet_buffer.getvalue(),\n",
    ")\n",
    "    print(f'completed file {i} and uploaded to S3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
